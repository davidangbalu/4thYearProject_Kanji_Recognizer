{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "federal-insider",
   "metadata": {},
   "source": [
    "# 1. Initialization & Reading of the Dataset\n",
    "\n",
    "#### Downloading the ETL9 Dataset and enclosing it into one npz file. This file will be used to read and train the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unique-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def read_record_ETL9(f):\n",
    "    s = f.read(8199)\n",
    "    r = struct.unpack('>2H8sI4B4H2B30x8128s11x', s)\n",
    "    iF = Image.frombytes('F', (128, 127), r[14], 'bit', 4)\n",
    "    iL = iF.convert('L')\n",
    "    return r + (iL,)\n",
    "\n",
    "def read_kanji():\n",
    "    kanji = np.zeros([883, 160, 127, 128], dtype=np.uint8)\n",
    "    for i in range(1, 33):\n",
    "        filename = 'ETL8G/ETL9_{:02d}'.format(i)\n",
    "        with open(filename, 'rb') as f:\n",
    "            for dataset in range(5):\n",
    "                char = 0\n",
    "                for j in range(956):\n",
    "                    r = read_record_ETL8G(f)\n",
    "                    if not (b'.HIRA' in r[2] or b'.WO.' in r[2]): \n",
    "                        kanji[char, (i - 1) * 5 + dataset] = np.array(r[-1])\n",
    "                        char += 1\n",
    "    np.savez_compressed(\"kanji.npz\", kanji)\n",
    "\n",
    "read_kanji()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-catholic",
   "metadata": {},
   "source": [
    "# 2. Modifying the Dataset\n",
    "\n",
    "#### Splitting the dataset into 4 parts:\n",
    "- training images\n",
    "- training labels\n",
    "- testing images\n",
    "- testing labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "graduate-finger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.transform\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "kanji = 879\n",
    "rows = 64\n",
    "cols = 64\n",
    "\n",
    "kan = np.load(\"kanji.npz\")['arr_0'].reshape([-1, 127, 128]).astype(np.float32)\n",
    "\n",
    "kan = kan/np.max(kan)\n",
    "\n",
    "train_images = np.zeros([kanji * 160, rows, cols], dtype=np.float32)\n",
    "\n",
    "arr = np.arange(kanji)\n",
    "train_labels = np.repeat(arr, 160)\n",
    "\n",
    "for i in range( (kanji+4) * 160):\n",
    "        if int(i/160) != 88 and int(i/160) != 219 and int(i/160) != 349 and int(i/160) != 457:\n",
    "            if int(i/160) < 88:\n",
    "                train_images[i] = skimage.transform.resize(kan[i], (rows, cols))\n",
    "            if int(i/160) > 88 and int(i/160) < 219:\n",
    "                train_images[i-160] = skimage.transform.resize(kan[i], (rows, cols))\n",
    "            if int(i/160) > 219 and int(i/160) < 349:\n",
    "                train_images[i-320] = skimage.transform.resize(kan[i], (rows, cols))\n",
    "            if int(i/160) > 349 and int(i/160) < 457:\n",
    "                if int(i/160) > 457:\n",
    "                    train_images[i-640] = skimage.transform.resize(kan[i], (rows, cols))\n",
    "      \n",
    "train_images, test_images, train_labels, test_labels = train_test_split(train_images, train_labels, test_size=0.2)\n",
    "\n",
    "np.savez_compressed(\"kanji_train_images.npz\", train_images)\n",
    "np.savez_compressed(\"kanji_train_labels.npz\", train_labels)\n",
    "np.savez_compressed(\"kanji_test_images.npz\", test_images)\n",
    "np.savez_compressed(\"kanji_test_labels.npz\", test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-mongolia",
   "metadata": {},
   "source": [
    "# 3. Creating the Model \n",
    "\n",
    "#### Here we begin loading in the training and test files and start to train the model. \n",
    "#### The process will take about 12 hours. (Set callbacks.EarlyStopping so that best epoch can be reached without continuing the training).\n",
    "#### Once finished, a .h5 file will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acute-closing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3516/3516 [==============================] - 1295s 368ms/step - loss: 5.2747 - accuracy: 0.1267 - val_loss: 3.9380 - val_accuracy: 0.3558\n",
      "Epoch 2/50\n",
      "3516/3516 [==============================] - 1297s 369ms/step - loss: 4.0428 - accuracy: 0.3291 - val_loss: 3.8709 - val_accuracy: 0.3749\n",
      "Epoch 3/50\n",
      "3516/3516 [==============================] - 1298s 369ms/step - loss: 3.9608 - accuracy: 0.3494 - val_loss: 3.8532 - val_accuracy: 0.3787\n",
      "Epoch 4/50\n",
      "3516/3516 [==============================] - 1296s 369ms/step - loss: 3.9139 - accuracy: 0.3613 - val_loss: 3.8451 - val_accuracy: 0.3824\n",
      "Epoch 5/50\n",
      "3516/3516 [==============================] - 1300s 370ms/step - loss: 3.8982 - accuracy: 0.3663 - val_loss: 3.8335 - val_accuracy: 0.3848\n",
      "Epoch 6/50\n",
      "3516/3516 [==============================] - 1300s 370ms/step - loss: 3.9001 - accuracy: 0.3689 - val_loss: 3.8330 - val_accuracy: 0.3857\n",
      "Epoch 7/50\n",
      "3516/3516 [==============================] - 1293s 368ms/step - loss: 3.8872 - accuracy: 0.3717 - val_loss: 3.8294 - val_accuracy: 0.3874\n",
      "Epoch 8/50\n",
      "3516/3516 [==============================] - 1293s 368ms/step - loss: 3.8741 - accuracy: 0.3740 - val_loss: 3.8294 - val_accuracy: 0.3869\n",
      "Epoch 9/50\n",
      "3516/3516 [==============================] - 1294s 368ms/step - loss: 3.8677 - accuracy: 0.3768 - val_loss: 3.8266 - val_accuracy: 0.3879\n",
      "Epoch 10/50\n",
      "3516/3516 [==============================] - 1300s 370ms/step - loss: 3.8589 - accuracy: 0.3784 - val_loss: 3.8259 - val_accuracy: 0.3877\n",
      "Epoch 11/50\n",
      "3516/3516 [==============================] - 1295s 368ms/step - loss: 3.8469 - accuracy: 0.3808 - val_loss: 3.8238 - val_accuracy: 0.3890\n",
      "Epoch 12/50\n",
      "3516/3516 [==============================] - 1296s 368ms/step - loss: 3.8490 - accuracy: 0.3805 - val_loss: 3.8297 - val_accuracy: 0.3874\n",
      "Epoch 13/50\n",
      "3516/3516 [==============================] - 1339s 381ms/step - loss: 3.8401 - accuracy: 0.3827 - val_loss: 3.8227 - val_accuracy: 0.3896\n",
      "Epoch 14/50\n",
      "3516/3516 [==============================] - 1506s 428ms/step - loss: 3.8437 - accuracy: 0.3828 - val_loss: 3.8268 - val_accuracy: 0.3894\n",
      "Epoch 15/50\n",
      "3516/3516 [==============================] - 1509s 429ms/step - loss: 3.8453 - accuracy: 0.3827 - val_loss: 3.8191 - val_accuracy: 0.3906\n",
      "Epoch 16/50\n",
      "3516/3516 [==============================] - 1443s 411ms/step - loss: 3.8364 - accuracy: 0.3841 - val_loss: 3.8206 - val_accuracy: 0.3901\n",
      "Epoch 17/50\n",
      "3516/3516 [==============================] - 1298s 369ms/step - loss: 3.8362 - accuracy: 0.3842 - val_loss: 3.8191 - val_accuracy: 0.3905\n",
      "Epoch 18/50\n",
      "3516/3516 [==============================] - 1303s 371ms/step - loss: 3.8469 - accuracy: 0.3827 - val_loss: 3.8199 - val_accuracy: 0.3905\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 19/50\n",
      "3516/3516 [==============================] - 1299s 369ms/step - loss: 3.8258 - accuracy: 0.3882 - val_loss: 3.8148 - val_accuracy: 0.3922\n",
      "Epoch 20/50\n",
      "3516/3516 [==============================] - 1303s 371ms/step - loss: 3.8206 - accuracy: 0.3891 - val_loss: 3.8134 - val_accuracy: 0.3925\n",
      "Epoch 21/50\n",
      "3516/3516 [==============================] - 1307s 372ms/step - loss: 3.8254 - accuracy: 0.3889 - val_loss: 3.8156 - val_accuracy: 0.3921\n",
      "Epoch 22/50\n",
      "3516/3516 [==============================] - 1299s 369ms/step - loss: 3.8087 - accuracy: 0.3920 - val_loss: 3.8153 - val_accuracy: 0.3921\n",
      "Epoch 23/50\n",
      "3516/3516 [==============================] - 1301s 370ms/step - loss: 3.8304 - accuracy: 0.3883 - val_loss: 3.8112 - val_accuracy: 0.3933\n",
      "Epoch 24/50\n",
      "3516/3516 [==============================] - 1302s 370ms/step - loss: 3.8331 - accuracy: 0.3881 - val_loss: 3.8153 - val_accuracy: 0.3920\n",
      "Epoch 25/50\n",
      "3516/3516 [==============================] - 1301s 370ms/step - loss: 3.8340 - accuracy: 0.3878 - val_loss: 3.8132 - val_accuracy: 0.3926\n",
      "Epoch 26/50\n",
      "3516/3516 [==============================] - 1302s 370ms/step - loss: 3.8201 - accuracy: 0.3900 - val_loss: 3.8145 - val_accuracy: 0.3925\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 27/50\n",
      "3516/3516 [==============================] - 1308s 372ms/step - loss: 3.8173 - accuracy: 0.3910 - val_loss: 3.8117 - val_accuracy: 0.3933\n",
      "Epoch 28/50\n",
      "3516/3516 [==============================] - 1297s 369ms/step - loss: 3.8260 - accuracy: 0.3899 - val_loss: 3.8112 - val_accuracy: 0.3932\n",
      "Epoch 29/50\n",
      "3516/3516 [==============================] - 1300s 370ms/step - loss: 3.8127 - accuracy: 0.3922 - val_loss: 3.8098 - val_accuracy: 0.3937\n",
      "Epoch 30/50\n",
      "3516/3516 [==============================] - 1295s 368ms/step - loss: 3.8082 - accuracy: 0.3930 - val_loss: 3.8115 - val_accuracy: 0.3930\n",
      "Epoch 31/50\n",
      "3516/3516 [==============================] - 1298s 369ms/step - loss: 3.8088 - accuracy: 0.3929 - val_loss: 3.8121 - val_accuracy: 0.3930\n",
      "Epoch 32/50\n",
      "3516/3516 [==============================] - 1302s 370ms/step - loss: 3.8106 - accuracy: 0.3925 - val_loss: 3.8102 - val_accuracy: 0.3936\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 33/50\n",
      "3516/3516 [==============================] - 1302s 370ms/step - loss: 3.8023 - accuracy: 0.3937 - val_loss: 3.8089 - val_accuracy: 0.3938\n",
      "Epoch 34/50\n",
      "3516/3516 [==============================] - 1301s 370ms/step - loss: 3.8109 - accuracy: 0.3931 - val_loss: 3.8071 - val_accuracy: 0.3940\n",
      "Epoch 35/50\n",
      "3516/3516 [==============================] - 1299s 369ms/step - loss: 3.8052 - accuracy: 0.3937 - val_loss: 3.8086 - val_accuracy: 0.3938\n",
      "Epoch 36/50\n",
      "3516/3516 [==============================] - 1297s 369ms/step - loss: 3.7897 - accuracy: 0.3963 - val_loss: 3.8097 - val_accuracy: 0.3936\n",
      "Epoch 37/50\n",
      "3516/3516 [==============================] - 1299s 370ms/step - loss: 3.7977 - accuracy: 0.3953 - val_loss: 3.8084 - val_accuracy: 0.3939\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 38/50\n",
      "3516/3516 [==============================] - 1298s 369ms/step - loss: 3.8094 - accuracy: 0.3935 - val_loss: 3.8077 - val_accuracy: 0.3940\n",
      "Epoch 39/50\n",
      "3516/3516 [==============================] - 1293s 368ms/step - loss: 3.8276 - accuracy: 0.3903 - val_loss: 3.8077 - val_accuracy: 0.3942\n",
      "Epoch 40/50\n",
      "3516/3516 [==============================] - 1298s 369ms/step - loss: 3.7985 - accuracy: 0.3954 - val_loss: 3.8072 - val_accuracy: 0.3942\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 41/50\n",
      "3516/3516 [==============================] - 1297s 369ms/step - loss: 3.8108 - accuracy: 0.3932 - val_loss: 3.8072 - val_accuracy: 0.3942\n",
      "Epoch 42/50\n",
      "3516/3516 [==============================] - 1302s 370ms/step - loss: 3.8028 - accuracy: 0.3946 - val_loss: 3.8071 - val_accuracy: 0.3941\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00042: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "\n",
    "train_images = np.load(\"kanji_train_images.npz\")['arr_0']\n",
    "train_labels = np.load(\"kanji_train_labels.npz\")['arr_0']\n",
    "test_images = np.load(\"kanji_test_images.npz\")['arr_0']\n",
    "test_labels = np.load(\"kanji_test_labels.npz\")['arr_0']\n",
    "\n",
    "if K.image_data_format() == \"channels_first\":\n",
    "  train_images = train_images.reshape(train_images.shape[0], 1,64,64)\n",
    "  test_images = test_images.reshape(test_images.shape[0], 1,64,64)\n",
    "  shape = (1,64,64)\n",
    "else:\n",
    "  train_images = train_images.reshape(train_images.shape[0], 64, 64, 1)\n",
    "  test_images = test_images.reshape(test_images.shape[0], 64, 64, 1)\n",
    "  shape = (64,64,1)\n",
    "  \n",
    "datagen = ImageDataGenerator(rotation_range=15,zoom_range=0.2)\n",
    "datagen.fit(train_images)\n",
    "model = keras.Sequential([\n",
    "  keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=shape),\n",
    "  keras.layers.MaxPooling2D(2,2),\n",
    "  keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "  keras.layers.MaxPooling2D(2,2),\n",
    "  keras.layers.Flatten(),\n",
    "  keras.layers.Dropout(0.5),\n",
    "  keras.layers.Dense(2048, activation='relu'),\n",
    "  keras.layers.Dense(879, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
    "              \n",
    "model.fit_generator(datagen.flow(train_images,train_labels,shuffle=True),epochs=50,validation_data=(test_images,test_labels),callbacks = [keras.callbacks.EarlyStopping(patience=8,verbose=1,restore_best_weights=True),keras.callbacks.ReduceLROnPlateau(factor=0.5,patience=3,verbose=1)])\n",
    "\n",
    "model.save(\"kanji.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-folder",
   "metadata": {},
   "source": [
    "# 4. Convert to Tensorflow Lite\n",
    "\n",
    "#### Now that the .h5 model has been created, we need to convert it to a format that is better suited for small devices such as a smartphone.\n",
    "#### We will convert the .h5 model to .tflite file so it can be used in an Android Application for a visual output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "collect-device",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:434: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpowwinwe1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpowwinwe1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpowwinwe1/variables/variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpowwinwe1/variables/variables\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'__saved_model_init_op', 'serving_default'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'__saved_model_init_op', 'serving_default'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input tensors info: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input tensors info: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: conv2d_input\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: conv2d_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow: tensor name: serving_default_conv2d_input:0, shape: (-1, 64, 64, 1), type: DT_FLOAT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow: tensor name: serving_default_conv2d_input:0, shape: (-1, 64, 64, 1), type: DT_FLOAT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:output tensors info: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:output tensors info: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: dense_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: dense_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow: tensor name: StatefulPartitionedCall:0, shape: (-1, 879), type: DT_FLOAT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow: tensor name: StatefulPartitionedCall:0, shape: (-1, 879), type: DT_FLOAT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpowwinwe1/variables/variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpowwinwe1/variables/variables\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "110126300"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(\"kanji2.h5\")\n",
    "\n",
    "converter.experimental_new_converter = True\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "open(\"kanji.tflite\", \"wb\").write(tflite_model)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
